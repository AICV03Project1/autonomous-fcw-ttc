{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNk65AFdsYF5Y0r1d8/j2f6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azK4f36gIXoO","executionInfo":{"status":"ok","timestamp":1768464427257,"user_tz":-540,"elapsed":3397,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"62c874e4-1f5f-44c0-d344-eecc37cf9ca7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/colab_data/Capture.zip\" /content"],"metadata":{"id":"EFRL_vJyd624","executionInfo":{"status":"ok","timestamp":1768464501249,"user_tz":-540,"elapsed":820,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["!unzip -q \"/content/Capture.zip\" -d /content/my_dataset"],"metadata":{"id":"gm9Rxw2ZIZGa","executionInfo":{"status":"ok","timestamp":1768464525122,"user_tz":-540,"elapsed":17891,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"61a4b3cb-c297-4cad-befd-b37862752a15"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["replace /content/my_dataset/bb_1_140613_vehicle_224_55460.mp4_20260114_143204.270.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["import os\n","import glob\n","import cv2\n","import shutil\n","import pandas as pd\n","\n","\n","def prepare_yolo_dataset_by_csv(root_path, csv_path, output_path):\n","    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n","    df = df[['env', 'split']].dropna()\n","    df = df[df['split'].isin(['train', 'val', 'test'])]\n","\n","    split_map = dict(zip(df['env'], df['split']))\n","    print(f\"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(split_map)}ê°œì˜ í™˜ê²½(env) ì„¤ì • ì½ìŒ.\")\n","\n","    image_paths = glob.glob(os.path.join(root_path, \"**/img/*.png\"), recursive=True)\n","    print(f\"ì´ {len(image_paths)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n","\n","    if len(image_paths) == 0:\n","        print(\"âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. root_path í™•ì¸ í•„ìš”\")\n","        return\n","\n","    for split in ['train', 'val', 'test']:\n","        os.makedirs(os.path.join(output_path, split, 'images'), exist_ok=True)\n","        os.makedirs(os.path.join(output_path, split, 'labels'), exist_ok=True)\n","\n","    processed_count = 0\n","    missing_env_count = 0\n","\n","    for img_path in image_paths:\n","        folder_name = img_path.split(os.sep)[-3]\n","        file_name = os.path.basename(img_path)\n","\n","        target_split = split_map.get(folder_name)\n","        if not target_split:\n","            missing_env_count += 1\n","            continue\n","\n","        new_base = f\"{folder_name}_{file_name}\"\n","        new_label = new_base.replace(\".png\", \".txt\")\n","\n","        org_label_path = img_path.replace(\"img\", \"new_txt\").replace(\".png\", \".txt\")\n","        if not os.path.exists(org_label_path):\n","            continue\n","\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            continue\n","        h, w, _ = img.shape\n","\n","        yolo_labels = []\n","        with open(org_label_path) as f:\n","            for line in f:\n","                data = list(map(float, line.split()))\n","                if len(data) < 5:\n","                    continue\n","\n","                x1, y1, x2, y2 = data[:4]\n","                class_id = int(data[4])\n","\n","                xc = ((x1 + x2) / 2) / w\n","                yc = ((y1 + y2) / 2) / h\n","                nw = (x2 - x1) / w\n","                nh = (y2 - y1) / h\n","\n","                yolo_labels.append(\n","                    f\"{class_id} {xc:.6f} {yc:.6f} {nw:.6f} {nh:.6f}\"\n","                )\n","\n","        if yolo_labels:\n","            shutil.copy(\n","                img_path,\n","                os.path.join(output_path, target_split, \"images\", new_base)\n","            )\n","            with open(\n","                os.path.join(output_path, target_split, \"labels\", new_label), \"w\"\n","            ) as f:\n","                f.write(\"\\n\".join(yolo_labels))\n","\n","            processed_count += 1\n","\n","    print(\"-\" * 30)\n","    print(\"âœ¨ ì²˜ë¦¬ ì™„ë£Œ!\")\n","    print(f\"- ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ëœ ì´ë¯¸ì§€: {processed_count}ì¥\")\n","    print(f\"- CSVì— ì •ì˜ë˜ì§€ ì•Šì•„ ì œì™¸ëœ í´ë”: {missing_env_count}ê°œ\")\n","    print(f\"- ê²°ê³¼ ì €ì¥ì†Œ: {os.path.abspath(output_path)}\")\n"],"metadata":{"id":"kSGA1eJNIZJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prepare_yolo_dataset_by_csv(\n","    root_path=\"/content/my_dataset\",\n","    csv_path=\"/content/drive/MyDrive/colab_data/AI_Project/split_assignment.csv\",\n","    output_path=\"/content/my_car_dataset\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0iwQzoaIZLn","executionInfo":{"status":"ok","timestamp":1768439974473,"user_tz":-540,"elapsed":649756,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"b644ea30-041b-409d-91cf-7fea1e9adc5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… CSV ë¡œë“œ ì™„ë£Œ: 165ê°œì˜ í™˜ê²½(env) ì„¤ì • ì½ìŒ.\n","ì´ 33873ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n","------------------------------\n","âœ¨ ì²˜ë¦¬ ì™„ë£Œ!\n","- ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ëœ ì´ë¯¸ì§€: 33873ì¥\n","- CSVì— ì •ì˜ë˜ì§€ ì•Šì•„ ì œì™¸ëœ í´ë”: 0ê°œ\n","- ê²°ê³¼ ì €ì¥ì†Œ: /content/my_car_dataset\n"]}]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYIuLg9-I1ua","executionInfo":{"status":"ok","timestamp":1768440212390,"user_tz":-540,"elapsed":9920,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"2f284c77-7c86-4c25-871e-85ee44c6f2a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.4.2-py3-none-any.whl.metadata (36 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n","Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n","Downloading ultralytics-8.4.2-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.4.2 ultralytics-thop-2.0.18\n"]}]},{"cell_type":"code","source":["#2-2) ëª¨ë¸ ì •ì˜ (ResNet50-FPN + ROIAlign + shared head + 2 heads)\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n","from torchvision.ops import MultiScaleRoIAlign\n","from torchvision.models.detection.faster_rcnn import TwoMLPHead\n","from torchvision.models.detection.transform import GeneralizedRCNNTransform\n","\n","def build_backbone_resnet50_fpn():\n","    # torchvision ë²„ì „ í˜¸í™˜ ì²˜ë¦¬\n","    try:\n","        from torchvision.models import ResNet50_Weights\n","        return resnet_fpn_backbone(\"resnet50\", weights=ResNet50_Weights.DEFAULT)\n","    except Exception:\n","        # êµ¬ë²„ì „ì—ì„œëŠ” pretrained ì¸ìë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ\n","        return resnet_fpn_backbone(\"resnet50\", pretrained=True)\n","\n","class ROIClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        num_location=5,\n","        num_action=4,\n","        pos_weight=None,                 # torch.Tensor[4]\n","        min_size=640, max_size=1024,      # transform resize\n","        roi_output_size=7,\n","        roi_sampling_ratio=2,\n","        representation_size=1024,\n","        image_mean=(0.485,0.456,0.406),\n","        image_std=(0.229,0.224,0.225),\n","    ):\n","        super().__init__()\n","\n","        self.transform = GeneralizedRCNNTransform(\n","            min_size=min_size,\n","            max_size=max_size,\n","            image_mean=list(image_mean),\n","            image_std=list(image_std),\n","        )\n","\n","        self.backbone = build_backbone_resnet50_fpn()\n","        out_channels = self.backbone.out_channels  # usually 256\n","\n","        self.roi_pooler = MultiScaleRoIAlign(\n","            featmap_names=[\"0\",\"1\",\"2\",\"3\"],\n","            output_size=roi_output_size,\n","            sampling_ratio=roi_sampling_ratio,\n","        )\n","\n","        self.box_head = TwoMLPHead(\n","            in_channels=out_channels * roi_output_size * roi_output_size,\n","            representation_size=representation_size,\n","        )\n","\n","        self.location_head = nn.Linear(representation_size, num_location)\n","        self.action_head   = nn.Linear(representation_size, num_action)\n","\n","        # action lossìš© pos_weight ì €ì¥\n","        if pos_weight is not None:\n","            self.register_buffer(\"pos_weight\", pos_weight.clone().detach())\n","        else:\n","            self.pos_weight = None\n","\n","    def forward(self, images, targets=None):\n","        \"\"\"\n","        images: list[Tensor(3,H,W)]\n","        targets: list[dict] with keys:\n","          boxes: FloatTensor[N,4] xyxy (ì›ë³¸ í”½ì…€ ì¢Œí‘œê³„)\n","          location: LongTensor[N]\n","          action: FloatTensor[N,4] (0/1)\n","        \"\"\"\n","        if self.training and targets is None:\n","            raise ValueError(\"Training requires targets\")\n","\n","        image_list, targets = self.transform(images, targets)\n","        features = self.backbone(image_list.tensors)  # dict[str,Tensor]\n","\n","        boxes = [t[\"boxes\"] for t in targets]\n","        pooled = self.roi_pooler(features, boxes, image_list.image_sizes)  # [sumN, C, S, S]\n","        pooled = pooled.flatten(start_dim=1)                               # [sumN, C*S*S]\n","        rep = self.box_head(pooled)                                        # [sumN, repr]\n","\n","        loc_logits = self.location_head(rep)  # [sumN, 5]\n","        act_logits = self.action_head(rep)    # [sumN, 4]\n","\n","        if self.training:\n","            loc_t = torch.cat([t[\"location\"] for t in targets], dim=0)  # [sumN]\n","            act_t = torch.cat([t[\"action\"] for t in targets], dim=0)    # [sumN,4]\n","\n","            loss_loc = F.cross_entropy(loc_logits, loc_t)\n","\n","            if self.pos_weight is None:\n","                loss_act = F.binary_cross_entropy_with_logits(act_logits, act_t)\n","            else:\n","                loss_act = F.binary_cross_entropy_with_logits(\n","                    act_logits, act_t, pos_weight=self.pos_weight\n","                )\n","\n","            return {\"loss_location\": loss_loc, \"loss_action\": loss_act}\n","\n","        # inference\n","        return F.softmax(loc_logits, dim=1), torch.sigmoid(act_logits)\n"],"metadata":{"id":"v7C9Ba6LIZOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import os\n","import cv2\n","import torch\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image as PILImage\n","import torchvision.transforms.functional as TF\n","\n","# --- 1. ì„¤ì • ---\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","dataset_root = '/content/my_dataset'\n","output_root = '/content/SafeDrive_T4_HighPerf/test_sequences_combined'\n","os.makedirs(output_root, exist_ok=True)\n","\n","# ì„±ë¯¼ë‹˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ â˜…\n","TEAMMATE_WEIGHTS_PATH = \"/content/drive/MyDrive/colab_data/roi.pt\"\n","\n","# --- 2. ëª¨ë¸ ë¡œë“œ ---\n","# (1) YOLO ëª¨ë¸ ë¡œë“œ\n","if 'model' in globals():\n","    yolo_model = model\n","    print(\"âœ… YOLO ëª¨ë¸(ë©”ëª¨ë¦¬) ì‚¬ìš©\")\n","else:\n","    from ultralytics import YOLO\n","    yolo_path = '/content/drive/MyDrive/colab_data/yolov8nv1_best.pt'\n","    if os.path.exists(yolo_path):\n","        yolo_model = YOLO(yolo_path)\n","        print(\"âœ… YOLO ëª¨ë¸ ìƒˆë¡œ ë¡œë“œ ì™„ë£Œ\")\n","    else:\n","        raise FileNotFoundError(\"âŒ YOLO ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","# (2) ROI ëª¨ë¸(íŒ€ì› ëª¨ë¸) ë¡œë“œ\n","print(f\"ğŸ“‚ íŒ€ì› ëª¨ë¸ ë¡œë“œ ì‹œë„: {TEAMMATE_WEIGHTS_PATH}\")\n","if not os.path.exists(TEAMMATE_WEIGHTS_PATH):\n","    raise FileNotFoundError(\"âŒ íŒ€ì› ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","\n","try:\n","    # ëª¨ë¸ êµ¬ì¡° ìƒì„± (ROIClassifier í´ë˜ìŠ¤ê°€ ë©”ëª¨ë¦¬ì— ìˆì–´ì•¼ í•¨)\n","    roi_model = ROIClassifier(num_location=5, num_action=4).to(device)\n","\n","    # ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ\n","    ckpt = torch.load(TEAMMATE_WEIGHTS_PATH, map_location=device, weights_only=False)\n","\n","    # state_dict í‚¤ ì²˜ë¦¬\n","    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n","        state_dict = ckpt[\"model_state\"]\n","    else:\n","        state_dict = ckpt\n","\n","    # ë¡œë“œ (strict=Falseë¡œ ì„¤ì •í•˜ì—¬ pos_weight ë“± ë¶ˆì¼ì¹˜ ë¬´ì‹œ)\n","    roi_model.load_state_dict(state_dict, strict=False)\n","    roi_model.eval()\n","    print(\"âœ… íŒ€ì› ëª¨ë¸(ROI) ë¡œë“œ ì„±ê³µ!\")\n","except Exception as e:\n","    print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","    raise e\n","\n","# --- 3. ì‹œí€€ìŠ¤ ê·¸ë£¹í™” ---\n","all_test_images = glob.glob(os.path.join(dataset_root,  'bb*.png'))\n","sequences = {}\n","\n","for img_path in all_test_images:\n","    filename = os.path.basename(img_path)\n","    try:\n","        # íŒŒì¼ëª…ì—ì„œ ì‹œí€€ìŠ¤ ì´ë¦„ ì¶”ì¶œ\n","        seq_name = filename.rsplit('_', 1)[0]\n","        if seq_name not in sequences:\n","            sequences[seq_name] = []\n","        sequences[seq_name].append(img_path)\n","    except IndexError:\n","        pass\n","\n","print(f\"ğŸ“‚ ì´ {len(sequences)}ê°œì˜ ì‹œí€€ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. í†µí•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n","\n","# --- 4. ë‚´ë¶€ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ---\n","@torch.no_grad()\n","def process_single_frame(image_path, yolo_model, roi_model, device):\n","    img_cv = cv2.imread(image_path)\n","    if img_cv is None: return None, []\n","\n","    filename = os.path.basename(image_path)\n","\n","    # YOLO Input\n","    img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n","    img_pil = PILImage.fromarray(img_rgb)\n","\n","    # YOLO Inference\n","    yolo_results = yolo_model(img_rgb, verbose=False)\n","    boxes = yolo_results[0].boxes.xyxy.cpu()\n","    clss = yolo_results[0].boxes.cls.cpu().int()\n","    names = yolo_model.names\n","\n","    if len(boxes) == 0: return img_cv, []\n","\n","    # ROI Input\n","    img_tensor = TF.to_tensor(img_pil).to(device)\n","    target_dict = [{\"boxes\": boxes.to(device)}]\n","\n","    # ROI Inference\n","    loc_probs, act_probs = roi_model([img_tensor], target_dict)\n","    loc_preds = loc_probs.argmax(dim=1).cpu().numpy()\n","    act_probs = act_probs.cpu().numpy()\n","\n","    # Config\n","    LOC_NAMES = [\"My\", \"Next\", \"Opposite\", \"RoadSide\", \"None\"]\n","    ACT_NAMES = [\"Brake\", \"Left\", \"Right\", \"Hazard\"]\n","\n","    frame_detections = []\n","    output_img = img_cv.copy()\n","\n","    for i, box in enumerate(boxes.numpy()):\n","        x1, y1, x2, y2 = map(int, box)\n","        cls_id = int(clss[i])\n","        class_name = names[cls_id] if names and cls_id in names else str(cls_id)\n","\n","        loc_idx = int(loc_preds[i])\n","        loc_text = LOC_NAMES[loc_idx] if loc_idx < len(LOC_NAMES) else str(loc_idx)\n","\n","        # Actions\n","        act_vals = act_probs[i]\n","        is_active = (act_vals > 0.5).astype(int)\n","        actions_str = [ACT_NAMES[j] for j, active in enumerate(is_active) if active]\n","        act_text = \",\".join(actions_str) if actions_str else \"Normal\"\n","\n","        # Append Data\n","        frame_detections.append({\n","            \"Filename\": filename,\n","            \"Class_Name\": class_name,\n","            \"Location_Name\": loc_text,\n","            \"Action_String\": act_text,\n","            \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2,\n","            \"Class_ID\": cls_id,\n","            \"Location_ID\": loc_idx,\n","            \"Brake\": is_active[0], \"Left\": is_active[1], \"Right\": is_active[2], \"Hazard\": is_active[3]\n","        })\n","\n","        # Draw\n","        label = f\"[{class_name}] {loc_text}|{act_text}\"\n","        cv2.rectangle(output_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","        (w_t, h_t), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n","        cv2.rectangle(output_img, (x1, y1 - 20), (x1 + w_t, y1), (0, 255, 0), -1)\n","        cv2.putText(output_img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n","\n","    return output_img, frame_detections\n","\n","# --- 5. ì „ì²´ ì‹œí€€ìŠ¤ ì‹¤í–‰ ---\n","for seq_name, img_list in tqdm(sequences.items(), desc=\"Total Progress\"):\n","    sorted_images = sorted(img_list)\n","    if not sorted_images: continue\n","\n","    # ì €ì¥ ê²½ë¡œ\n","    seq_save_dir = os.path.join(output_root, seq_name)\n","    os.makedirs(seq_save_dir, exist_ok=True)\n","\n","    video_path = os.path.join(seq_save_dir, f\"{seq_name}_combined.mp4\")\n","    csv_path = os.path.join(seq_save_dir, f\"{seq_name}_combined.csv\")\n","\n","    # Video Init\n","    sample = cv2.imread(sorted_images[0])\n","    h, w = sample.shape[:2]\n","    out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 20.0, (w, h))\n","\n","    seq_detections = []\n","\n","    for img_path in sorted_images:\n","        res_img, dets = process_single_frame(img_path, yolo_model, roi_model, device)\n","        if res_img is not None:\n","            out.write(res_img)\n","            seq_detections.extend(dets)\n","\n","    out.release()\n","\n","    if seq_detections:\n","        df = pd.DataFrame(seq_detections)\n","        cols = [\"Filename\", \"x1\", \"y1\", \"x2\", \"y2\", \"Class_ID\", \"Location_ID\", \"Brake\", \"Left\", \"Right\", \"Hazard\", \"Class_Name\", \"Location_Name\", \"Action_String\"]\n","        exist_cols = [c for c in cols if c in df.columns]\n","        df[exist_cols].to_csv(csv_path, index=False, encoding='utf-8-sig')\n","\n","print(f\"\\nâœ¨ ëª¨ë“  í†µí•© ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ ê²½ë¡œ: {output_root}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9W-zVMbxTUT7","executionInfo":{"status":"ok","timestamp":1768464559213,"user_tz":-540,"elapsed":10270,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"9be8470d-0453-4eb8-9c14-ae5eea376bca"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… YOLO ëª¨ë¸ ìƒˆë¡œ ë¡œë“œ ì™„ë£Œ\n","ğŸ“‚ íŒ€ì› ëª¨ë¸ ë¡œë“œ ì‹œë„: /content/drive/MyDrive/colab_data/roi.pt\n","âœ… íŒ€ì› ëª¨ë¸(ROI) ë¡œë“œ ì„±ê³µ!\n","ğŸ“‚ ì´ 2ê°œì˜ ì‹œí€€ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. í†µí•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"]},{"output_type":"stream","name":"stderr","text":["Total Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.41s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ¨ ëª¨ë“  í†µí•© ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ ê²½ë¡œ: /content/SafeDrive_T4_HighPerf/test_sequences_combined\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kL2b7rXSqe4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Install ---\n","!pip install numpy filterpy scipy imageio tqdm\n","!git clone https://github.com/abewley/sort.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9AycpaEIZTb","executionInfo":{"status":"ok","timestamp":1768461426890,"user_tz":-540,"elapsed":9788,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"8baa5bf9-de3d-4987-c797-bf0ca6d2cb8a"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: filterpy in /usr/local/lib/python3.12/dist-packages (1.4.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from filterpy) (3.10.0)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio) (11.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n","Cloning into 'sort'...\n","remote: Enumerating objects: 208, done.\u001b[K\n","remote: Counting objects: 100% (49/49), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 208 (delta 45), reused 40 (delta 40), pack-reused 159 (from 1)\u001b[K\n","Receiving objects: 100% (208/208), 1.20 MiB | 5.90 MiB/s, done.\n","Resolving deltas: 100% (76/76), done.\n"]}]},{"cell_type":"code","source":["%%writefile /content/sort_tracker.py\n","# =====================================================\n","# SORT: Simple Online and Realtime Tracking\n","# Source adapted from https://github.com/abewley/sort\n","# =====================================================\n","\n","import numpy as np\n","from filterpy.kalman import KalmanFilter\n","from scipy.optimize import linear_sum_assignment\n","\n","\n","def iou(bb_test, bb_gt):\n","    xx1 = np.maximum(bb_test[0], bb_gt[0])\n","    yy1 = np.maximum(bb_test[1], bb_gt[1])\n","    xx2 = np.minimum(bb_test[2], bb_gt[2])\n","    yy2 = np.minimum(bb_test[3], bb_gt[3])\n","    w = np.maximum(0., xx2 - xx1)\n","    h = np.maximum(0., yy2 - yy1)\n","    wh = w * h\n","    o = wh / (\n","        (bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1]) +\n","        (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1]) - wh\n","    )\n","    return o\n","\n","\n","class KalmanBoxTracker:\n","    count = 0\n","\n","    def __init__(self, bbox):\n","        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n","        self.kf.F = np.array(\n","            [[1,0,0,0,1,0,0],\n","            [0,1,0,0,0,1,0],\n","            [0,0,1,0,0,0,1],\n","            [0,0,0,1,0,0,0],\n","            [0,0,0,0,1,0,0],\n","            [0,0,0,0,0,1,0],\n","            [0,0,0,0,0,0,1]]\n","        )\n","        self.kf.H = np.array([\n","            [1,0,0,0,0,0,0],\n","            [0,1,0,0,0,0,0],\n","            [0,0,1,0,0,0,0],\n","            [0,0,0,1,0,0,0]\n","        ])\n","        self.kf.R[2:,2:] *= 10.\n","        self.kf.P[4:,4:] *= 1000.\n","        self.kf.P *= 10.\n","        self.kf.Q[-1,-1] *= 0.01\n","        self.kf.Q[4:,4:] *= 0.01\n","\n","        # Fix: only take first 4 elements (coords)\n","        self.kf.x[:4] = bbox[:4].reshape((4,1))\n","        self.time_since_update = 0\n","        self.id = KalmanBoxTracker.count\n","        KalmanBoxTracker.count += 1\n","        self.history = []\n","        self.hits = 0\n","        self.hit_streak = 0\n","        self.age = 0\n","\n","    def update(self, bbox):\n","        self.time_since_update = 0\n","        self.history = []\n","        self.hits += 1\n","        self.hit_streak += 1\n","        # Fix: only take first 4 elements (coords)\n","        self.kf.update(bbox[:4].reshape((4,1)))\n","\n","    def predict(self):\n","        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n","            self.kf.x[6] *= 0.0\n","        self.kf.predict()\n","        self.age += 1\n","        if self.time_since_update > 0:\n","            self.hit_streak = 0\n","        self.time_since_update += 1\n","        self.history.append(self.kf.x)\n","        return self.kf.x\n","\n","\n","class Sort:\n","    def __init__(self, max_age=5, min_hits=2, iou_threshold=0.3):\n","        self.max_age = max_age\n","        self.min_hits = min_hits\n","        self.iou_threshold = iou_threshold\n","        self.trackers = []\n","        self.frame_count = 0\n","\n","    def update(self, dets=np.empty((0,4))):\n","        self.frame_count += 1\n","\n","        trks = np.zeros((len(self.trackers), 4))\n","        to_del = []\n","        for t, trk in enumerate(self.trackers):\n","            pos = trk.predict()\n","            trks[t] = pos[:4].reshape((1,4))\n","            if np.any(np.isnan(pos)):\n","                to_del.append(t)\n","        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n","        for t in reversed(to_del):\n","            self.trackers.pop(t)\n","\n","        matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(\n","            dets, trks, self.iou_threshold\n","        )\n","\n","        for m in matched:\n","            self.trackers[m[1]].update(dets[m[0]])\n","\n","        for i in unmatched_dets:\n","            self.trackers.append(KalmanBoxTracker(dets[i]))\n","\n","        ret = []\n","        for trk in self.trackers:\n","            if trk.time_since_update < 1 and (trk.hits >= self.min_hits or self.frame_count <= self.min_hits):\n","                ret.append(np.concatenate((trk.kf.x[:4].reshape((1,4)), [[trk.id]]), axis=1))\n","        return np.concatenate(ret) if len(ret) > 0 else np.empty((0,5))\n","\n","\n","def associate_detections_to_trackers(detections, trackers, iou_threshold):\n","    if len(trackers) == 0:\n","        return np.empty((0,2), dtype=int), np.arange(len(detections)), np.empty((0), dtype=int)\n","\n","    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)\n","    for d, det in enumerate(detections):\n","        for t, trk in enumerate(trackers):\n","            iou_matrix[d,t] = iou(det, trk)\n","\n","    matched_indices = linear_sum_assignment(-iou_matrix)\n","    matched_indices = np.array(list(zip(*matched_indices)))\n","\n","    # Fix for IndexError when matched_indices is empty\n","    if len(matched_indices) == 0:\n","        matched_indices = np.empty((0, 2), dtype=int)\n","\n","    unmatched_detections = []\n","    for d in range(len(detections)):\n","        if d not in matched_indices[:,0]:\n","            unmatched_detections.append(d)\n","\n","    unmatched_trackers = []\n","    for t in range(len(trackers)):\n","        if t not in matched_indices[:,1]:\n","            unmatched_trackers.append(t)\n","\n","    matches = []\n","    for m in matched_indices:\n","        if iou_matrix[m[0], m[1]] < iou_threshold:\n","            unmatched_detections.append(m[0])\n","            unmatched_trackers.append(m[1])\n","        else:\n","            matches.append(m.reshape(1,2))\n","\n","    if len(matches) == 0:\n","        matches = np.empty((0,2), dtype=int)\n","    else:\n","        matches = np.concatenate(matches, axis=0)\n","\n","    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvVcls0UMriJ","executionInfo":{"status":"ok","timestamp":1768461429740,"user_tz":-540,"elapsed":13,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"865cece4-60c2-4844-ba3d-cf1f240421e9"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/sort_tracker.py\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0, \"/content\")\n","\n","from sort_tracker import Sort\n","\n","tracker = Sort()\n","print(\"SORT import OK\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ny9L6FNoMr50","executionInfo":{"status":"ok","timestamp":1768461440596,"user_tz":-540,"elapsed":5,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"0c872537-8945-4e29-d7fb-78393e977925"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["SORT import OK\n"]}]},{"cell_type":"code","source":["import cv2\n","import os\n","import glob\n","import numpy as np\n","import imageio\n","from collections import deque\n","from tqdm import tqdm"],"metadata":{"id":"nRJrtCv0Mr9_","executionInfo":{"status":"ok","timestamp":1768461442561,"user_tz":-540,"elapsed":1,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["class IntentAwareRiskManager:\n","    def __init__(self, window_size=5):\n","        self.window_size = window_size\n","        self.history = {}  # {track_id: deque[(cx, cy)]}\n","\n","    def _update_history(self, track_id, center):\n","        if track_id not in self.history:\n","            self.history[track_id] = deque(maxlen=self.window_size)\n","        self.history[track_id].append(center)\n","\n","    def _is_long_static(self, centers, move_thresh=3.0, min_static_frames=6):\n","        if len(centers) < min_static_frames + 1:\n","            return False\n","\n","        diffs = [\n","            np.linalg.norm(np.array(centers[i]) - np.array(centers[i-1]))\n","            for i in range(1, len(centers))\n","        ]\n","        return all(d < move_thresh for d in diffs[-min_static_frames:])\n","\n","    def _compute_2d_traj_metrics(self, centers, fps, img_w, img_h, eps=1e-3):\n","        \"\"\"\n","        return: ttc, v_rel, d_curr\n","        \"\"\"\n","        if len(centers) < 2:\n","            return float('inf'), 0.0, float('inf')\n","\n","        ego = np.array([img_w / 2, img_h])\n","        p_prev = np.array(centers[-2])\n","        p_curr = np.array(centers[-1])\n","\n","        d_prev = np.linalg.norm(p_prev - ego)\n","        d_curr = np.linalg.norm(p_curr - ego)\n","        v_rel = (d_prev - d_curr) * fps  # px/sec\n","\n","        # ì ‘ê·¼ ì•ˆ í•˜ë©´ ë¬´íš¨\n","        if v_rel < 5.0:\n","            return float('inf'), v_rel, d_curr\n","\n","        # ì¥ê¸° ì •ì§€ í•„í„°\n","        if self._is_long_static(centers):\n","            return float('inf'), v_rel, d_curr\n","\n","        ttc = d_curr / max(v_rel, eps)\n","        return ttc, v_rel, d_curr\n","\n","    def calculate_risk(self, track_id, bbox, location, actions, fps, img_w, img_h):\n","        cx = (bbox[0] + bbox[2]) / 2\n","        cy = (bbox[1] + bbox[3]) / 2\n","        self._update_history(track_id, (cx, cy))\n","        centers = self.history[track_id]\n","\n","        # --- Step 1: Physical risk ---\n","        ttc, v_rel, d_curr = self._compute_2d_traj_metrics(\n","            centers, fps, img_w, img_h\n","        )\n","\n","        base_score = 0.0\n","        if ttc < 5.0:\n","            base_score = min(50.0, 50.0 * (3.0 / max(ttc, 0.5)))\n","\n","        # --- Step 2: Semantic reasoning ---\n","        semantic_bonus = 0.0\n","        omega = 1.0\n","        reason = \"Normal\"\n","\n","        # Ignore irrelevant zones\n","        if location in [2, 3, 4]:\n","            return {\n","                \"score\": 0.0,\n","                \"ttc\": \"Safe\",\n","                \"reason\": \"Ignored\",\n","                \"level\": \"SAFE\"\n","            }\n","\n","        HIGH_SPEED = 80.0  # px/sec\n","\n","        # Ego lane\n","        if location == 0:\n","            if ttc < 2.0 and v_rel > HIGH_SPEED:\n","                reason = \"Rapid Approach\"\n","                semantic_bonus = 10.0\n","            elif ttc < 2.0:\n","                reason = \"Close Following\"\n","            else:\n","                reason = \"Normal Deceleration\"\n","\n","        # Adjacent lane (cut-in)\n","        elif location == 1:\n","            if actions[1] == 1 or actions[2] == 1:\n","                reason = \"Dangerous Cut-in\"\n","                semantic_bonus = 10.0\n","                omega = 1.3\n","            else:\n","                reason = \"Approaching\"\n","                omega = 0.8\n","\n","        final_risk = min(base_score * omega + semantic_bonus, 100.0)\n","\n","        return {\n","            \"score\": round(final_risk, 2),\n","            \"ttc\": round(ttc, 2) if ttc != float('inf') else \"Safe\",\n","            \"reason\": reason,\n","            \"level\": self._get_risk_level(final_risk),\n","        }\n","\n","    def _get_risk_level(self, score):\n","        if score > 50: return \"DANGER\"\n","        if score > 20: return \"WARNING\"\n","        return \"SAFE\"\n"],"metadata":{"id":"bkSbCaxyMsAl","executionInfo":{"status":"ok","timestamp":1768461444103,"user_tz":-540,"elapsed":3,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b8b7967","executionInfo":{"status":"ok","timestamp":1768461453303,"user_tz":-540,"elapsed":4177,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"8405bd99-23af-41d5-d699-36cb91e0945a"},"source":["import cv2\n","import numpy as np\n","import torch\n","import os\n","from tqdm import tqdm\n","from PIL import Image as PILImage\n","import torchvision.transforms.functional as TF\n","\n","# --- RELOAD SORT TO APPLY FIX ---\n","import importlib\n","import sort_tracker\n","importlib.reload(sort_tracker)\n","from sort_tracker import Sort\n","\n","# Ensure dependencies are loaded\n","if 'IntentAwareRiskManager' not in globals():\n","    # Re-define or ensure cell is run. Assuming it is run based on context.\n","    pass\n","\n","# --- Color Palette for Risk Levels ---\n","RISK_COLORS = {\n","    \"SAFE\": (0, 255, 0),       # Green\n","    \"WARNING\": (0, 165, 255),  # Orange\n","    \"DANGER\": (0, 0, 255)      # Red\n","}\n","\n","def process_sequence_with_risk(seq_name, image_paths, output_dir, yolo_model, roi_model, device):\n","    os.makedirs(output_dir, exist_ok=True)\n","    video_path = os.path.join(output_dir, f\"{seq_name}_risk_analysis.mp4\")\n","\n","    # Sort images to ensure temporal order\n","    image_paths = sorted(image_paths)\n","    if not image_paths: return\n","\n","    # Init Tracker & Risk Manager per sequence\n","    # min_hits=1: ê°ì§€ ì¦‰ì‹œ íŠ¸ë˜í‚¹ ì‹œì‘ (ë°˜ì‘ ì†ë„ ìµœìš°ì„ )\n","    tracker = Sort(max_age=5, min_hits=1, iou_threshold=0.3)\n","    risk_manager = IntentAwareRiskManager(window_size=10)\n","\n","    # Video Writer Init\n","    sample_img = cv2.imread(image_paths[0])\n","    h, w = sample_img.shape[:2]\n","    fps = 10.0 # Assumption for playback speed\n","    out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n","\n","    print(f\"ğŸ¬ Processing {seq_name} ({len(image_paths)} frames)...\")\n","\n","    # Use torch.no_grad() for inference to save memory and avoid grad errors\n","    with torch.no_grad():\n","        for img_path in tqdm(image_paths, leave=False):\n","            img_cv = cv2.imread(img_path)\n","            if img_cv is None: continue\n","\n","            # [Safety Fix] Resize frame if dimensions mismatch\n","            if img_cv.shape[:2] != (h, w):\n","                img_cv = cv2.resize(img_cv, (w, h))\n","\n","            # 1. YOLO Detection\n","            img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n","            results = yolo_model(img_rgb, verbose=False)\n","\n","            dets_to_sort = np.empty((0, 5))\n","            if results[0].boxes.id is not None:\n","                 # If YOLO has tracking, ignore it, we use SORT. Just take boxes & conf\n","                 pass\n","\n","            # Extract boxes for SORT [x1, y1, x2, y2, score]\n","            # We use confidence as score\n","            boxes = results[0].boxes.xyxy.cpu().numpy()\n","            confs = results[0].boxes.conf.cpu().numpy()\n","            if len(boxes) > 0:\n","                dets_to_sort = np.column_stack((boxes, confs))\n","\n","            # 2. SORT Tracking\n","            # track_dets: [x1, y1, x2, y2, track_id]\n","            track_dets = tracker.update(dets_to_sort)\n","\n","            # 3. ROI Classification & Risk Calculation\n","            if len(track_dets) > 0:\n","                # Prepare inputs for ROI model (using tracked boxes)\n","                # ROI model expects boxes in a list of dicts\n","                roi_boxes_tensor = torch.tensor(track_dets[:, :4]).float().to(device)\n","                img_tensor = TF.to_tensor(PILImage.fromarray(img_rgb)).to(device)\n","\n","                # ROI Forward\n","                # Note: We process all tracked objects in batch\n","                roi_loc_probs, roi_act_probs = roi_model([img_tensor], [{\"boxes\": roi_boxes_tensor}])\n","\n","                # Use .detach() just in case, though torch.no_grad() handles most cases\n","                loc_preds = roi_loc_probs.argmax(dim=1).detach().cpu().numpy()\n","                act_probs = roi_act_probs.detach().cpu().numpy()\n","\n","                LOC_NAMES = [\"My\", \"Next\", \"Opposite\", \"RoadSide\", \"None\"]\n","                ACT_NAMES = [\"Brake\", \"Left\", \"Right\", \"Hazard\"]\n","\n","                for i, trk in enumerate(track_dets):\n","                    x1, y1, x2, y2, track_id = map(int, trk)\n","\n","                    # Get ROI results\n","                    loc_idx = loc_preds[i]\n","                    act_vals = act_probs[i]\n","\n","                    # Decode ROI\n","                    loc_name = LOC_NAMES[loc_idx] if loc_idx < len(LOC_NAMES) else \"Unknown\"\n","                    is_active = (act_vals > 0.5).astype(int)\n","\n","                    # 4. Risk Calculation\n","                    risk_result = risk_manager.calculate_risk(\n","                        track_id=track_id,\n","                        bbox=[x1, y1, x2, y2],\n","                        location=loc_idx,\n","                        actions=is_active,\n","                        fps=fps,\n","                        img_w=w,\n","                        img_h=h\n","                    )\n","\n","                    # 5. Visualization\n","                    risk_level = risk_result[\"level\"]\n","                    risk_score = risk_result[\"score\"]\n","                    reason = risk_result[\"reason\"]\n","                    color = RISK_COLORS.get(risk_level, (0, 255, 0))\n","\n","                    # Bounding Box\n","                    cv2.rectangle(img_cv, (x1, y1), (x2, y2), color, 2)\n","\n","                    # Info Box\n","                    info_text = f\"ID:{track_id} | {loc_name}\"\n","                    risk_text = f\"{risk_level} ({risk_score})\"\n","                    reason_text = f\"{reason}\"\n","\n","                    # Draw labels\n","                    cv2.putText(img_cv, info_text, (x1, y1 - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","                    cv2.putText(img_cv, risk_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","                    if risk_level != \"SAFE\":\n","                         cv2.putText(img_cv, reason_text, (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","\n","            out.write(img_cv)\n","\n","    out.release()\n","\n","# --- Run on All Sequences ---\n","OUTPUT_ROOT_RISK = \"/content/SafeDrive_T4_HighPerf/integrated_risk_videos\"\n","\n","# 'sequences' variable should be available from previous cells\n","# If not, we regenerate it briefly:\n","if 'sequences' not in globals() or not sequences:\n","    print(\"âš ï¸ sequences not found, rescanning...\")\n","    test_imgs = glob.glob(os.path.join('/content/my_car_dataset', 'test', 'images', '*.png'))\n","    sequences = {}\n","    for p in test_imgs:\n","        s_name = os.path.basename(p).rsplit('_', 1)[0]\n","        sequences.setdefault(s_name, []).append(p)\n","\n","print(f\"ğŸš€ Starting Risk Analysis on {len(sequences)} sequences...\")\n","\n","for seq_name, img_list in sequences.items():\n","    process_sequence_with_risk(\n","        seq_name,\n","        img_list,\n","        OUTPUT_ROOT_RISK,\n","        yolo_model,\n","        roi_model,\n","        device\n","    )\n","\n","print(f\"\\nâœ… Done! Check results in: {OUTPUT_ROOT_RISK}\")"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Starting Risk Analysis on 1 sequences...\n","ğŸ¬ Processing bb_1_140613_vehicle_224_55460.mp4_20260114 (29 frames)...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Done! Check results in: /content/SafeDrive_T4_HighPerf/integrated_risk_videos\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import glob\n","from tqdm import tqdm\n","from collections import deque\n","from sort_tracker import Sort\n","\n","# --- ì„¤ì • ë° ê²½ë¡œ ---\n","INPUT_ROOT = '/content/SafeDrive_T4_HighPerf/test_sequences_combined'\n","IMAGE_ROOT = '/content/my_dataset'\n","OUTPUT_ROOT = '/content/SafeDrive_T4_HighPerf/final_risk_result_v4'\n","os.makedirs(OUTPUT_ROOT, exist_ok=True)\n","\n","FOCAL_LENGTH = 800\n","ACTUAL_WIDTH = 1.8\n","FPS = 10.0\n","\n","# --- IoU ê³„ì‚° ---\n","def get_iou(bb1, bb2):\n","    x_left = max(bb1[0], bb2[0])\n","    y_top = max(bb1[1], bb2[1])\n","    x_right = min(bb1[2], bb2[2])\n","    y_bottom = min(bb1[3], bb2[3])\n","    if x_right < x_left or y_bottom < y_top: return 0.0\n","    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n","    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n","    return intersection_area / float(bb1_area + bb2_area - intersection_area)\n","\n","# --- ìœ„í—˜ë„ íŒë‹¨ ---\n","def calculate_risk_advanced(dist, ttc, loc, lights, is_static):\n","    base_score = 0\n","    reasons = []\n","\n","    if is_static: return 0, [\"Static(Safe)\"]\n","    if loc >= 4: return 0, [\"Ignored\"]\n","\n","    # ë¬¼ë¦¬ì  ìœ„í—˜\n","    if dist <= 5.0: base_score, reasons = 80, [\"Crit.Dist\"]\n","    elif ttc < 2.0: base_score, reasons = 75, [f\"Crit.TTC({ttc:.1f}s)\"]\n","    elif dist <= 10.0: base_score, reasons = 50, [\"Close\"]\n","    elif ttc < 4.0: base_score, reasons = 40, [f\"Appr({ttc:.1f}s)\"]\n","    elif dist <= 20.0: base_score, reasons = 20, [\"Near\"]\n","    final_score = base_score\n","\n","    # ë¸Œë ˆì´í¬\n","    if lights['Brake'] == 1:\n","        if loc == 0: final_score += 30; reasons.append(\"Brake\")\n","        elif loc == 1: final_score += 15; reasons.append(\"SideBrake\")\n","\n","    # ë¹„ìƒë“±\n","    if lights['Hazard'] == 1: final_score += 20; reasons.append(\"Hazard\")\n","\n","    # ë¼ì–´ë“¤ê¸°\n","    if (lights['Left'] == 1 or lights['Right'] == 1) and loc == 1:\n","        if dist < 15.0 or ttc < 5.0: final_score += 15; reasons.append(\"Cut-in\")\n","        else: final_score += 5\n","\n","    return min(final_score, 100), reasons\n","\n","# --- ì´ë¯¸ì§€ ì¸ë±ì‹± ---\n","def index_images(root_dir):\n","    print(f\"ğŸ” ì´ë¯¸ì§€ ê²½ë¡œ ì¸ë±ì‹± ì¤‘... ({root_dir})\")\n","    image_map = {}\n","    count = 0\n","    for root, dirs, files in os.walk(root_dir):\n","        for file in files:\n","            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n","                image_map[file] = os.path.join(root, file)\n","                count += 1\n","    print(f\"âœ… ì´ {count}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n","    return image_map\n","\n","# --- ë©”ì¸ ì²˜ë¦¬ ---\n","def process_combined_sequences():\n","    # 0. ì´ë¯¸ì§€ ì¸ë±ì‹±\n","    image_map = index_images(IMAGE_ROOT)\n","\n","    # 1. ì‹œí€€ìŠ¤ CSV ì°¾ê¸°\n","    # í´ë” êµ¬ì¡°: INPUT_ROOT / seq_name / seq_name_combined.csv\n","    # í˜¹ì€ íŒŒì¼ì´ ë°”ë¡œ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì¬ê·€ íƒìƒ‰\n","    csv_files = glob.glob(os.path.join(INPUT_ROOT, '**', '*combined.csv'), recursive=True)\n","    print(f\"ğŸ“‚ ì´ {len(csv_files)}ê°œì˜ í†µí•© CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\")\n","\n","    for csv_path in csv_files:\n","        try:\n","            seq_name = os.path.basename(os.path.dirname(csv_path))\n","            # ë§Œì•½ í´ë”ëª…ì´ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ\n","            if not seq_name or seq_name == os.path.basename(INPUT_ROOT):\n","                 seq_name = os.path.basename(csv_path).replace('_combined.csv', '')\n","\n","            print(f\"ğŸš€ Processing Sequence: {seq_name}\")\n","\n","            df = pd.read_csv(csv_path)\n","            if df.empty:\n","                print(\"âš ï¸ CSVê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n","                continue\n","\n","            df = df.sort_values(by='Filename')\n","            unique_files = df['Filename'].unique()\n","\n","            if len(unique_files) == 0:\n","                print(\"âš ï¸ ì²˜ë¦¬í•  í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","                continue\n","\n","            # ì €ì¥ ê²½ë¡œ ì„¤ì •\n","            save_video_path = os.path.join(OUTPUT_ROOT, f\"{seq_name}_risk_final.mp4\")\n","            save_frames_dir = os.path.join(OUTPUT_ROOT, f\"{seq_name}_frames\")\n","            os.makedirs(save_frames_dir, exist_ok=True)\n","\n","            # ì²« í”„ë ˆì„ í™•ì¸ ë° VideoWriter ì´ˆê¸°í™”\n","            first_file = unique_files[0]\n","            if first_file not in image_map:\n","                print(f\"âŒ ì²« í”„ë ˆì„({first_file})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","                # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ê²½ë¡œ ì§ì ‘ í™•ì¸\n","                continue\n","\n","            sample_img = cv2.imread(image_map[first_file])\n","            if sample_img is None:\n","                print(f\"âŒ ì´ë¯¸ì§€ ì½ê¸° ì‹¤íŒ¨: {image_map[first_file]}\")\n","                continue\n","\n","            h, w = sample_img.shape[:2]\n","            out = cv2.VideoWriter(save_video_path, cv2.VideoWriter_fourcc(*'mp4v'), FPS, (w, h))\n","\n","            tracker = Sort(max_age=5, min_hits=3, iou_threshold=0.3)\n","            dist_history = {}\n","\n","            processed_frames = 0\n","            for filename in tqdm(unique_files, desc=f\"Seq: {seq_name}\", leave=False):\n","                if filename not in image_map:\n","                    continue\n","\n","                img_path = image_map[filename]\n","                frame = cv2.imread(img_path)\n","                if frame is None: continue\n","\n","                frame_data = df[df['Filename'] == filename]\n","\n","                # SORT ì…ë ¥\n","                dets_for_sort = []\n","                original_attrs = []\n","                for _, row in frame_data.iterrows():\n","                    dets_for_sort.append([row['x1'], row['y1'], row['x2'], row['y2'], 1.0])\n","                    original_attrs.append(row)\n","\n","                dets_for_sort = np.array(dets_for_sort)\n","                if len(dets_for_sort) == 0:\n","                    dets_for_sort = np.empty((0, 5))\n","\n","                tracked_objs = tracker.update(dets_for_sort)\n","\n","                for trk in tracked_objs:\n","                    tx1, ty1, tx2, ty2, trk_id = map(int, trk[:5])\n","                    trk_box = [tx1, ty1, tx2, ty2]\n","\n","                    # ë§¤ì¹­\n","                    best_iou, matched_row = 0, None\n","                    for row in original_attrs:\n","                        org_box = [row['x1'], row['y1'], row['x2'], row['y2']]\n","                        iou = get_iou(trk_box, org_box)\n","                        if iou > best_iou:\n","                            best_iou, matched_row = iou, row\n","\n","                    if matched_row is None or best_iou < 0.3: continue\n","\n","                    # ê±°ë¦¬ & TTC\n","                    w_pixel = tx2 - tx1\n","                    current_dist = (FOCAL_LENGTH * ACTUAL_WIDTH) / w_pixel if w_pixel > 0 else 0\n","\n","                    if trk_id not in dist_history: dist_history[trk_id] = deque(maxlen=5)\n","                    dist_history[trk_id].append(current_dist)\n","\n","                    is_static = False\n","                    recent = dist_history[trk_id]\n","                    if len(recent) >= 3 and all(abs(recent[i]-recent[i-1]) < 0.5 for i in range(1, len(recent))):\n","                        is_static = True\n","\n","                    ttc = float('inf')\n","                    if len(recent) >= 2:\n","                        v_rel = (recent[-2] - current_dist) * FPS\n","                        if v_rel > 0.1: ttc = current_dist / v_rel\n","\n","                    # ìœ„í—˜ë„\n","                    lights = {k: matched_row[k] for k in ['Brake', 'Left', 'Right', 'Hazard']}\n","                    score, reasons = calculate_risk_advanced(\n","                        current_dist, ttc, int(matched_row['Location_ID']), lights, is_static\n","                    )\n","\n","                    # ì‹œê°í™”\n","                    color = (0, 0, 255) if score >= 80 else (0, 165, 255) if score >= 50 else (0, 255, 0)\n","                    cv2.rectangle(frame, (tx1, ty1), (tx2, ty2), color, 2)\n","\n","                    info = f\"ID:{trk_id} [{matched_row['Class_Name']}] {matched_row['Location_Name']}\"\n","                    risk = f\"{'DANGER' if score>=80 else 'WARN' if score>=50 else 'Safe'} ({score})\"\n","                    dist_info = f\"{current_dist:.1f}m | TTC:{ttc:.1f}s\" if ttc != float('inf') else f\"{current_dist:.1f}m | TTC:Inf\"\n","\n","                    cv2.putText(frame, info, (tx1, ty1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","                    cv2.putText(frame, risk, (tx1, ty1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","                    cv2.putText(frame, dist_info, (tx1, ty2+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)\n","                    if reasons and score > 0:\n","                        cv2.putText(frame, \",\".join(reasons), (tx1, ty2+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n","\n","                out.write(frame)\n","                cv2.imwrite(os.path.join(save_frames_dir, filename), frame)\n","                processed_frames += 1\n","\n","            out.release()\n","            print(f\"   âœ… ì™„ë£Œ: {processed_frames}ì¥ ì²˜ë¦¬ë¨ -> {save_video_path}\")\n","\n","        except Exception as e:\n","            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ ({seq_name}): {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","    print(f\"\\nâœ¨ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ í™•ì¸: {OUTPUT_ROOT}\")\n","\n","process_combined_sequences()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0Fw8JObAsyO","executionInfo":{"status":"ok","timestamp":1768464632240,"user_tz":-540,"elapsed":8097,"user":{"displayName":"â€ì„ì€ì„(í•™ë¶€ìƒ-ê¸°ê³„ê³µí•™ì „ê³µ)","userId":"06808069550326772552"}},"outputId":"d7a3f176-bbaa-4f06-c4d5-7b134fb6bce7"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” ì´ë¯¸ì§€ ê²½ë¡œ ì¸ë±ì‹± ì¤‘... (/content/my_dataset)\n","âœ… ì´ 135540ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n","ğŸ“‚ ì´ 18ê°œì˜ í†µí•© CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n","ğŸš€ Processing Sequence: ETRINear_Fog\n","âŒ ì²« í”„ë ˆì„(ETRINear_Fog_00000797.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_140613_vehicle_224_55460.mp4_20260114\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   âœ… ì™„ë£Œ: 29ì¥ ì²˜ë¦¬ë¨ -> /content/SafeDrive_T4_HighPerf/final_risk_result_v4/bb_1_140613_vehicle_224_55460.mp4_20260114_risk_final.mp4\n","ğŸš€ Processing Sequence: bb_1_210701_vehicle_217_21650.mp4\n","âŒ ì²« í”„ë ˆì„(bb_1_210701_vehicle_217_21650.mp4_12.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: TailLight04\n","âŒ ì²« í”„ë ˆì„(TailLight04_000044.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_211026_vehicle_144_301.mp4\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   âœ… ì™„ë£Œ: 19ì¥ ì²˜ë¦¬ë¨ -> /content/SafeDrive_T4_HighPerf/final_risk_result_v4/bb_1_211026_vehicle_144_301.mp4_risk_final.mp4\n","ğŸš€ Processing Sequence: TailLight57\n","âŒ ì²« í”„ë ˆì„(TailLight57_000020.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_150402_vehicle_142_073.mp4\n","âŒ ì²« í”„ë ˆì„(bb_1_150402_vehicle_142_073.mp4_0.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: OisamtoBanseok_Snow\n","âŒ ì²« í”„ë ˆì„(OisamtoBanseok_Snow_00021609.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: RAIN_Noeun\n","âŒ ì²« í”„ë ˆì„(RAIN_Noeun_000090.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: CounterClock2_Fog\n","âŒ ì²« í”„ë ˆì„(CounterClock2_Fog_00000368.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_200714_vehicle_200_28840.mp4\n","âŒ ì²« í”„ë ˆì„(bb_1_200714_vehicle_200_28840.mp4_10.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: Yeonhwi\n","âŒ ì²« í”„ë ˆì„(Yeonhwi_000009.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: TailLight27\n","âŒ ì²« í”„ë ˆì„(TailLight27_000036.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_160612_vehicle_212_53256.mp4\n","âŒ ì²« í”„ë ˆì„(bb_1_160612_vehicle_212_53256.mp4_8.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: TailLight87\n","âŒ ì²« í”„ë ˆì„(TailLight87_000000.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_210522_vehicle_315_061.mp4\n","âŒ ì²« í”„ë ˆì„(bb_1_210522_vehicle_315_061.mp4_0.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: bb_1_210114_vehicle_253_50073.mp4\n","âŒ ì²« í”„ë ˆì„(bb_1_210114_vehicle_253_50073.mp4_0.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","ğŸš€ Processing Sequence: TailLight45\n","âŒ ì²« í”„ë ˆì„(TailLight45_000010.png)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n","\n","âœ¨ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ í™•ì¸: /content/SafeDrive_T4_HighPerf/final_risk_result_v4\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ToYYO4dJIZVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tOs_6i1rIZYY"},"execution_count":null,"outputs":[]}]}